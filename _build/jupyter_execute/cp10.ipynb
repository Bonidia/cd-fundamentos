{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26251b6",
   "metadata": {},
   "source": [
    "# Dados Não Estruturados\n",
    "\n",
    "```{admonition} Importante\n",
    ":class: tip\n",
    "Para execução dos códigos, é necessário instalar as seguintes bibliotecas: \n",
    "\n",
    " - *!pip install -U scikit-learn*\n",
    " - *!pip install biopython*\n",
    " - *!pip install -U scikit-image*\n",
    " - *!pip install --no-binary :all: opencv-python*\n",
    " - *!pip install spaCy*\n",
    " - *!pip install -U spacy-lookups-data*\n",
    " - *!python -m spacy download pt_core_news_lg*\n",
    " - *!pip install nltk*\n",
    "```\n",
    "\n",
    "## Análise de Sequências Biológicas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72a656a4",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Importando Bibliotecas necessárias\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "# Importando Bibliotecas necessárias\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import collections\n",
    "from Bio import SeqIO\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def janela_deslizante(seq, janela):\n",
    "    seqlen = len(seq)\n",
    "    for i in range(seqlen):\n",
    "        j = seqlen if i+janela>seqlen else i+janela\n",
    "        yield seq[i:j]\n",
    "        if j==seqlen: break\n",
    "    return\n",
    "\n",
    "\n",
    "def k_possiveis(tamanho_k, tipo_seq):\n",
    "    k_possiveis = [''.join(str(i) for i in x) for x in product(tipo_seq, repeat=tamanho_k)]\n",
    "    dados_estruturados = pd.DataFrame(columns=range(len(k_possiveis)))\n",
    "    dados_estruturados.columns = k_possiveis\n",
    "\n",
    "    kmer = {}\n",
    "    for k in k_possiveis:\n",
    "        kmer[k] = 0\n",
    "    kmer = collections.OrderedDict(sorted(kmer.items()))\n",
    "    return dados_estruturados, kmer\n",
    "\n",
    "\n",
    "def kmers(entrada, tamanho_k, tipo_seq):\n",
    "    \n",
    "    dados_estruturados, kmer = k_possiveis(tamanho_k, tipo_seq)\n",
    "   \n",
    "    for seq_record in SeqIO.parse(entrada, \"fasta\"):\n",
    "        seq = seq_record.seq\n",
    "        seq = seq.upper()   \n",
    "\n",
    "        for subseq in janela_deslizante(seq, tamanho_k):\n",
    "            try:\n",
    "              kmer[subseq] = kmer[subseq] + 1\n",
    "            except:\n",
    "              pass \n",
    "\n",
    "        dados_estruturados = dados_estruturados.append(kmer, ignore_index=True)\n",
    "        kmer = dict.fromkeys(kmer, 0)\n",
    "    return dados_estruturados\n",
    "    \n",
    "    \n",
    "dna = ['A', 'C', 'G', 'T']\n",
    "rna = ['A', 'C', 'G', 'U']\n",
    "protein = ['A', 'C', 'D', 'E', 'F', \n",
    "           'G', 'H', 'I', 'K', 'L', \n",
    "           'M', 'N', 'P', 'Q', 'R', \n",
    "           'S', 'T', 'V', 'W', 'Y']\n",
    "\n",
    "pos_estruturado = kmers('positivo.fasta', 1, protein)\n",
    "print(pos_estruturado)\n",
    "\n",
    "neg_estruturado = kmers('negativo.fasta', 1, protein)\n",
    "print(neg_estruturado)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373e2be5",
   "metadata": {},
   "source": [
    "## Análise de Imagens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d0550d",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "# Importando bibliotecas para carregar e manipular os dados\n",
    "import os\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Diretorio base onde as imagens foram carregadas\n",
    "base_dir = \"./yalefaces\"\n",
    "\n",
    "# Leitura dos arquivos das imagens\n",
    "image_files = [os.path.join(base_dir, file) for file in os.listdir(base_dir)]\n",
    "images = [np.array(Image.open(img)) for img in image_files]\n",
    "\n",
    "# Inicialização do detector de faces Haar Cascades através do arquivo obtido no repositório do OpenCV\n",
    "faceDetectClassifier = cv2.CascadeClassifier(\"./haarcascade_frontalface_default.xml\")\n",
    "faces_cropped = []\n",
    "\n",
    "# Aplicação do detector de face e segmentando a imagem original para um tamanho de 150x150 contendo somente as faces\n",
    "for img in images:\n",
    "    facePoints = faceDetectClassifier.detectMultiScale(img)\n",
    "    x, y = facePoints[0][:2]\n",
    "    cropped = img[y: y + 150, x: x + 150]\n",
    "    faces_cropped.append(cropped)\n",
    "\n",
    "faces_cropped = np.array(faces_cropped)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,4))\n",
    "\n",
    "ax[0].imshow(images[0], cmap='gray')\n",
    "ax[1].imshow(faces_cropped[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7ace90",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "from skimage.feature import local_binary_pattern\n",
    "\n",
    "# Para evitar a divisão por um número próximo a zero\n",
    "eps = 1e-7\n",
    "\n",
    "feature_vector = []\n",
    "\n",
    "for face in faces_cropped:\n",
    "\n",
    "  # Obtenção da imagem com os padrões locais binários\n",
    "  lbp = local_binary_pattern(image=face, \n",
    "                             P=8, R=1, \n",
    "                             method='uniform')\n",
    "\n",
    "  # Representando e concatenando os padrões com um histograma\n",
    "  n_bins = int(lbp.max() + 1)\n",
    "  (hist, _) = np.histogram(lbp.ravel(),\n",
    "                           bins=n_bins,\n",
    "                           range=(0, n_bins))\n",
    "\n",
    "  # Normalizando o histograma obtido\n",
    "  hist = hist.astype(\"float\")\n",
    "  hist /= (hist.sum() + eps)\n",
    "\n",
    "  feature_vector.append(hist)\n",
    "\n",
    "feature_vector = np.array(feature_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a06f49",
   "metadata": {},
   "source": [
    "## Análise de Textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e3fb0a",
   "metadata": {
    "tags": [
     "remove-output"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy \n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "nltk.download('punkt')\n",
    "pln = spacy.load('pt_core_news_lg')\n",
    "\n",
    "def remove_acentos(df):\n",
    "    df = df.str.replace('[àáâãäå]', 'a')\n",
    "    df = df.str.replace('[èéêë]', 'e')\n",
    "    df = df.str.replace('[ìíîï]', 'i')\n",
    "    df = df.str.replace('[òóôõö]', 'o')\n",
    "    df = df.str.replace('[ùúûü]', 'u')\n",
    "    df = df.str.replace('[ç]', 'c')\n",
    "    return df\n",
    "\n",
    "def conversao_letras_minusculas(df):\n",
    "    df = df.str.lower()\n",
    "    return df\n",
    "\n",
    "def remove_pontuacao(df):\n",
    "    df = df.str.replace('[^\\w\\s]', '')\n",
    "    return df\n",
    "\n",
    "def remove_numeros(df):\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if not word.isdigit()]))\n",
    "    return df\n",
    "\n",
    "def remove_url(df):\n",
    "    df = df.str.replace('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '')\n",
    "    return df\n",
    "\n",
    "def remove_tag(df):\n",
    "    df = df.str.replace(r'<[^<>]*>', '', regex=True)\n",
    "    return df\n",
    "\n",
    "def remove_palavras_frequentes(df, n_palavras):\n",
    "    palavras = []\n",
    "    textos = df.apply(nltk.word_tokenize)\n",
    "    for texto in textos:\n",
    "        for palavra in texto:\n",
    "            palavras.append(palavra)\n",
    "    freq = [x for x in nltk.FreqDist(palavras)]\n",
    "    frequentes = freq[0:n_palavras]\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (frequentes)]))\n",
    "    return df\n",
    "\n",
    "def remove_palavras_raras(df, n_palavras):\n",
    "    palavras = []\n",
    "    textos = df.apply(nltk.word_tokenize)\n",
    "    for texto in textos:\n",
    "        for palavra in texto:\n",
    "            palavras.append(palavra)\n",
    "    freq = [x for x in nltk.FreqDist(palavras)]\n",
    "    raras = freq[-n_palavras:]\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (raras)]))\n",
    "    return df\n",
    "\n",
    "def palavras_vazias(df):\n",
    "    stopwords = pln.Defaults.stop_words\n",
    "    df = df.apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    return df\n",
    "\n",
    "def lematizacao(df):\n",
    "    df = df.apply(lambda x: ' '.join([word.lemma_ for word in pln(x)]))\n",
    "    return df\n",
    "\n",
    "def saco_de_palavras(df):\n",
    "    metodo = CountVectorizer(ngram_range=(1,1), stop_words=pln.Defaults.stop_words)\n",
    "    X = metodo.fit_transform(df)\n",
    "    df = pd.DataFrame(X.toarray(), columns=metodo.get_feature_names())\n",
    "    return df\n",
    "\n",
    "def tf_idf(df):\n",
    "    metodo = TfidfVectorizer()\n",
    "    X = metodo.fit_transform(df)\n",
    "    df = pd.DataFrame(X.toarray(), columns=metodo.get_feature_names())\n",
    "    return df\n",
    "\n",
    "\n",
    "df = pd.read_csv('FACTCKBR.tsv', sep='\\t')\n",
    "df = df.rename(columns={'title': 'titulo'})\n",
    "\n",
    "df['titulo'] = remove_acentos(df['titulo'])\n",
    "df['titulo'] = conversao_letras_minusculas(df['titulo'])\n",
    "df['titulo'] = remove_pontuacao(df['titulo'])\n",
    "df['titulo'] = remove_numeros(df['titulo'])\n",
    "df['titulo'] = remove_url(df['titulo'])\n",
    "df['titulo'] = remove_tag(df['titulo'])\n",
    "df['titulo'] = palavras_vazias(df['titulo'])\n",
    "df['titulo'] = lematizacao(df['titulo'])\n",
    "df['titulo'] = remove_palavras_frequentes(df['titulo'], 20)\n",
    "df['titulo'] = remove_palavras_raras(df['titulo'], 20)\n",
    "conjunto_final = tf_idf(df['titulo'])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst",
    "format_version": 0.13,
    "jupytext_version": "1.11.5"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "source_map": [
   13,
   34,
   96,
   101,
   136,
   165,
   170
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}